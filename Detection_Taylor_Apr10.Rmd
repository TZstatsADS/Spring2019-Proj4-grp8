---
title: 'Detection Part'
output: html_notebook
---

Xinyi Hu

```{r Load libraries}

library(stringr)
library(tm)
library(tidytext)
library(dplyr)

```

```{r read files}
#setwd("~/Desktop/GR5243 Applied Data Science/Project 4/project 4 edit")

project_path = "~/Desktop/GR5243 Applied Data Science/Project 4/Spring2019-Proj4-grp8-1"
true_path = "/data/ground_truth/"
OCR_path = "/data/tesseract/"

#true_path <- "../data/ground_truth/"
#OCR_path <- "../data/tesseract/"
#correctPath = "../output/correct/"

ground_truth = list.files(path=paste0(project_path,true_path), pattern="*.txt", full.names=TRUE, recursive=FALSE)
OCR = list.files(path=paste0(project_path,OCR_path), pattern="*.txt", full.names=TRUE, recursive=FALSE)

#if(length(truthFiles) != length(ocrFiles)) stop("the number of ground truth files is not equal to the number of OCR recognized files")

n = length(ground_truth) # number of files

# select train + validation and testing data 80% and 20%
set.seed(2019)
train = sample(1:n, size = 0.7*n, replace = F)

# set training data
truthTrain = ground_truth[train]
ocrTrain = OCR[train]

# set testing data
truthTest = ground_truth[-train]
ocrTest = OCR[-train]
```

```{r Extract Ground Truth}

Truth_Extract = function(filenames){

  filenames <- ground_truth[1:5]  # for debugging
  
  text = ""
  for (file in filenames){
    #file = filenames[1]
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
  }
  
  #text = "applied data science is challenging.!sfsfas 09073 \\W"
  text<-gsub("[[:punct:]]", " ", text)
  word = strsplit(text," ")[[1]]
  words = word[word!=""]
  #toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))

  corpus = VCorpus(VectorSource(words))%>%
    tm_map(stripWhitespace)%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removeWords, character(0))%>%
    tm_map(removeNumbers)%>%
    tm_map(removePunctuation)#%>%
    #tm_map(toSpace, "\\W")
  
  truth_words = tidy(corpus) %>%
    select(text) %>%
    unnest_tokens(dictionary, text)
    
  
  truth_words = unique(truth_words)
  truth_words = as.matrix(truth_words)
  truth_words = truth_words[nchar(truth_words) > 1] # no single character words
  #truth_words = truth_words[truth_words!=""]

  #check_punct = grep("[[:punct:]]", truth_words)
  #check_num = grep("[0-9]", truth_words)
  #check_punct
  #check_num

  
  return(truth_words)
}

#dictionary = Truth_Extract(ground_truth)

#save(dictionary, file = paste0(project_path,"/output/GroundTruthWords.Rdata"))

```

```{r Digrams short words}
short = "abc ab ba ac bac cba cca bba aab"
string = strsplit(short,split = " ")[[1]];string
N = max(nchar(string))
dic = list()

for (i in 2:N){     # empty dictionary digrams
   
  dic[[i]] = list()   # word length
  n = i*(i-1)/2      # number of distinct pairs in word with length i
  
  for (j in 1:n){
    
    dic[[i]][[j]] = matrix(0, nrow = 4, ncol = 4)
    
  }
}
dic[[1]] = "No single-character words"


for (word in string){
  
  n = nchar(word)
  count_pair = 1
  
  for (i in 1:(n-1)){
    for (j in 2:n){
      if (i<j){
        
        row_index = match(substr(word, i, i),letters)
        col_index = match(substr(word, j, j),letters)
        dic[[n]][[count_pair]][row_index,col_index] = 1
        count_pair = count_pair + 1
      }
    }
  }
}
dic[[2]]
dic[[3]]
```

```{r Digrams 36x36 26x26}
#text = "applied data science is challenging"
#string = strsplit(text,split = " ")[[1]];string
#N = max(nchar(string))

test_dic = dictionary[1:1000]
N = max(nchar(test_dic))
dic = list()
#numberletters = c(0:9, letters)

for (i in 2:N){     # empty dictionary digrams
  dic[[i]] = list()   # word length
  #cat(i, "\n")
  n = i*(i-1)/2      # number of distinct pairs in word with length i
  for (j in 1:n){
    #dic[[i]][[j]] = matrix(0, nrow = 36, ncol = 36)
    dic[[i]][[j]] = matrix(0, nrow = 26, ncol = 26)

  }
}
dic[[1]] = "No single-character words"
length(dic[[4]])   # Check
length(dic[[5]])

#for (word in string){
for (word in test_dic){
  
  n = nchar(word)
  count_pair = 1
  
  for (i in 1:(n-1)){
    
    for (j in 2:n){
      
      if (i<j){
        
        #row_index = match(substr(word, i, i),numberletters)
        #col_index = match(substr(word, j, j),numberletters)
        row_index = match(substr(word, i, i),letters)
        col_index = match(substr(word, j, j),letters)
        dic[[n]][[count_pair]][row_index,col_index] = 1
        count_pair = count_pair + 1
        
      }
    }
  }
}
#check
dic[[1]]
apply(dic[[5]][[4]],2,sum)
```

```{r Dic-Digrams}
Digrams = function(dictionary){
  
  dic = dictionary  # character vector
  
  N = max(nchar(dic))
  digrams = list()
  #numberletters = c(0:9, letters)

  for (i in 2:N){     # empty dictionary digrams
    
    digrams[[i]] = list()   # word length
    n = i*(i-1)/2      # number of distinct pairs in word with length i
    
    for (j in 1:n){
      #dic[[i]][[j]] = matrix(0, nrow = 36, ncol = 36)
      digrams[[i]][[j]] = matrix(0, nrow = 26, ncol = 26)
  
    }
  }
  digrams[[1]] = "No single-character words"
  #length(digrams) == N

  #for (word in string){
  for (word in dic){
  
    n = nchar(word)
    count_pair = 1
  
    for (i in 1:(n-1)){
    
      for (j in 2:n){
      
        if (i<j){
        
          #row_index = match(substr(word, i, i),numberletters)
          #col_index = match(substr(word, j, j),numberletters)
          row_index = match(substr(word, i, i),letters)
          col_index = match(substr(word, j, j),letters)
          digrams[[n]][[count_pair]][row_index,col_index] = 1
          count_pair = count_pair + 1
        
        }
      }
    }
  }

  # apply(digrams[[24]][[270]],2,sum)
  # length(digrams[[24]])

  
  return(digrams)
}

dic_digrams = Digrams(dictionary)

save(dic_digrams, file = paste0(project_path,"/output/digrams.Rdata"))

```



```{r Extract OCR}
  
  filenames = ocrTrain[1:5]
  text = ""
  for (file in filenames){
    
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
  }
  word = strsplit(text," ")[[1]]
  words = word[word!=""]
  n = length(words)
  
  
  ocr_words = c()
  i = 1            
  for (file in filenames){
    #file = filenames[1]
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
    #txt_word = unlist(strsplit(text," "))
    ocr_words[i] = text
    i = i+1               # count .txt 
  }
  
  ocr_txt_word = strsplit(ocr_words," ")   
  # each of 100 txts contains its own words, store in a single list
  
  for (i in 1:length(ocr_txt_word)){
    ocr_txt_word[[i]] = ocr_txt_word[[i]][ocr_txt_word[[i]]!=""]# clean null words
  }
  
  n1 = nchar(ocr_txt_word[[1]])
  sum(n1)
  
  n = sum(nchar(ocr_txt_word))
  
  word_map = matrix(NA, ncol = 4, nrow = n)
  colnames(word_map) = c("word", "word_index", "txt", "incorrect")
  
  i = 1
  for (t in 1:length(ocr_txt_word)){
  # t = 1
    for (w in 1:length(ocr_txt_word[[t]])){
   #   w = 2
      word_map[i,] = c(ocr_txt_word[[t]][w], w, t, 0)
      i = i+1
    }
  }
  see = as.data.frame(word_map)
  na.index = is.na(word_map[,1])
  ####################################################
  # create dataset of words

  # tokens -> list[[docs]] -> each [[doc]] -> [[lines]] -> each [[line]] -> [words]
  
  bag <- matrix(NA, ncol = 5, nrow = n, 
                dimnames = list(rep("", n), c("token", "word", "line", "doc", "error")))
  
  i <- 1
  for(d in 1:length(tokens)){ # each txt
    for(l in 1:length(tokens[[d]])){ # d th txt with n_d words
      for(w in 1:length(tokens[[d]][[l]])){ # extract each word in d th txt
        bag[i,] <- c(tokens[[d]][[l]][w], w, l, d, 0)  # c(exact word, word posi in line l, line l in doc d, doc d in tokens, error)
        i <- i + 1
      }
    }
  }
  
  #d = 1;l = 3
  #head(tokens[[d]])
  #tokens[[d]][[l]]
  #length(tokens[[d]][[l]])
  
  bag <- bag[complete.cases(bag),]
  length(bag)
  nrow(bag)
  
  # toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
  
  corpus <- VCorpus(VectorSource(bag[,1]))%>%
    tm_map(content_transformer(tolower))%>%
    # tm_map(toSpace, "\\W")%>%
    tm_map(removePunctuation)%>%
    tm_map(removeWords, character(0))%>%
    tm_map(stripWhitespace)
  
  library(textreg)
  b <- convert.tm.to.character(corpus)
  bag[,1] <- b
  
  assign(outName, bag)
  save(list=outName, file = paste0("../output/", outName, ".RData"))
  return(eval(as.name(paste(outName))))

```


```{r Extract OCR version 2}
  
  filenames = ocrTrain[1:5]
  #text = ""
  text = list()
  i = 1
  for (file in filenames){
    
    text[[i]] = readLines(file)
    #text_lines = readLines(file)
   # text = paste(text, 
    #              paste(text_lines, collapse = " ")
     #             )
    i = i+1
  }
  word = strsplit(text," ")[[1]]
  words = word[word!=""]
  n = length(words)
  
  
  ocr_words = c()
  i = 1            
  for (file in filenames){
    #file = filenames[1]
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
    #txt_word = unlist(strsplit(text," "))
    ocr_words[i] = text
    i = i+1               # count .txt 
  }
  
  ocr_txt_word = strsplit(ocr_words," ")   
  # each of 100 txts contains its own words, store in a single list
  
  for (i in 1:length(ocr_txt_word)){
    ocr_txt_word[[i]] = ocr_txt_word[[i]][ocr_txt_word[[i]]!=""]# clean null words
  }
  
  n1 = nchar(ocr_txt_word[[1]])
  sum(n1)
  
  n = sum(nchar(ocr_txt_word))
  
  word_map = matrix(NA, ncol = 4, nrow = n)
  colnames(word_map) = c("word", "word_index", "txt", "incorrect")
  
  i = 1
  for (t in 1:length(ocr_txt_word)){
  # t = 1
    for (w in 1:length(ocr_txt_word[[t]])){
   #   w = 2
      word_map[i,] = c(ocr_txt_word[[t]][w], w, t, 0)
      i = i+1
    }
  }
  see = as.data.frame(word_map)
  na.index = is.na(word_map[,1])
  ####################################################
  # create dataset of words

  # tokens -> list[[docs]] -> each [[doc]] -> [[lines]] -> each [[line]] -> [words]
  
  bag <- matrix(NA, ncol = 5, nrow = n, 
                dimnames = list(rep("", n), c("token", "word", "line", "doc", "error")))
  
  i <- 1
  for(d in 1:length(tokens)){ # each txt
    for(l in 1:length(tokens[[d]])){ # d th txt with n_d words
      for(w in 1:length(tokens[[d]][[l]])){ # extract each word in d th txt
        bag[i,] <- c(tokens[[d]][[l]][w], w, l, d, 0)  # c(exact word, word posi in line l, line l in doc d, doc d in tokens, error)
        i <- i + 1
      }
    }
  }
  
  #d = 1;l = 3
  #head(tokens[[d]])
  #tokens[[d]][[l]]
  #length(tokens[[d]][[l]])
  
  bag <- bag[complete.cases(bag),]
  length(bag)
  nrow(bag)
  
  # toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
  
  corpus <- VCorpus(VectorSource(bag[,1]))%>%
    tm_map(content_transformer(tolower))%>%
    # tm_map(toSpace, "\\W")%>%
    tm_map(removePunctuation)%>%
    tm_map(removeWords, character(0))%>%
    tm_map(stripWhitespace)
  
  library(textreg)
  b <- convert.tm.to.character(corpus)
  bag[,1] <- b
  
  assign(outName, bag)
  save(list=outName, file = paste0("../output/", outName, ".RData"))
  return(eval(as.name(paste(outName))))

```



