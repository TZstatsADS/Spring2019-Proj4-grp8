agg_class = function(X, alpha, allPars){
output = rep(0, nrow(X))
for (i in 1:length(alpha)){
c_i = classify(X, allPars[[i]])
output = output + c_i*alpha[i]
}
sign = ifelse(output > 0, 1, -1)
return(sign)
}
path = "~/Desktop/GR5241 Stat ML/HW/hw 5"
train_3 = read.table(paste0(path,"/train_3.txt"), header = FALSE, sep = ",")
train_8 = read.table(paste0(path,"/train_8.txt"), header = FALSE, sep = ",")
test = read.table("zip_test.txt", header = FALSE, sep = ",")
train_3$y = 1
train_8$y = -1
train_3 = as.matrix(train_3)
train_8 = as.matrix(train_8)
train_matrix = rbind(train_3,train_8)
test = as.data.frame(test)
clean.test = function(testrow){
row.data = as.character(unlist(strsplit(as.character(testrow), split = " ")))
y = row.data[1]
row.data = row.data[-1]
row.data = as.numeric(c(row.data,y))
return(row.data)
}
test.data = apply(test, 1, clean.test)
test.data = t(test.data)
dim(test.data)
test_data = as.data.frame(test.data)
test_data[which(test_data[,257] == 3),257] = 1
test_data[which(test_data[,257] == 8),257] = -1
index = which(test_data[,257] == 1 | test_data[,257] == -1)
test_matrix = as.matrix(test_data[index,])
#unique(test_matrix[,257])
pars = train(X = train_matrix[,-257],
w = 1/nrow(train_matrix),
train_matrix[,257])
label = classify(train_matrix[,-257], pars)
pars
round(table(True_y = train_matrix[,257],Predict_y = label)/length(label),3)
Adaboost = function(train_data = train_matrix, test_data = test_matrix,
B = 1000, K = 5, seed = 2019, show_status = F){
set.seed(seed)
index = sample(1:nrow(train_data),size = nrow(train_data), replace = F)
train_data = train_data[index,]
#train_data = train_data; test_data = test_matrix;B = 1000;K = 5  # Debug
w = rep(1/nrow(train_data), nrow(train_data))
allPars = list()
alpha = c()
training_error = c()
test_error = c()
for (b in 1:B){
#b = 1
valid_error = c()
cv_pars = list()
for (i in 1:K){    # Tune parameters
#i = 1
start = (i-1)*(nrow(train_data)%/%K)+1
end = i*(nrow(train_data)%/%K)
validation_index = c(start:end)
train_index = c(1:nrow(train_data))[-validation_index]
train_set = train_data[train_index,]
valid_set = train_data[validation_index,]
train_w = w[train_index]
valid_w = w[validation_index]
# Train a weak learner:
cv_pars[[i]] = train(X = train_set[,-257], w = train_w,
y = train_set[,257])
pred = classify(valid_set[,-257], cv_pars[[i]])
valid_error[i] = mean(pred != valid_set[,257])
}
if(show_status) cat("Weak learner",b,"is done.\n")
pick_pars = which.min(valid_error)
allPars[[b]] = cv_pars[[pick_pars]]
pred = classify(train_data[,-257],allPars[[b]])
error_b = sum(as.numeric(pred != train_data[,257])*w)/sum(w)
alpha[b] = log((1-error_b)/error_b)
# Update weights:
w = w*exp(alpha[b]*as.numeric(pred != train_data[,257]))
# Current classifier:
pred_train = agg_class(train_data[,-257], alpha = alpha, allPars = allPars)
training_error[b] = mean( pred_train != train_data[,257])
#training_error_[b] = sum(abs(pred_train - train_data[,257])/2)/length(pred_train)
pred_test = agg_class(test_data[,-257], alpha = alpha, allPars = allPars)
test_error[b] = mean( pred_test != test_data[,257])
#test_error_[b] = sum(abs(pred_test - test_data[,257])/2)/length(pred_test)
#see_pred = cbind(pred_test,test_data[,257])
#head(see_pred, 100)
}
return(list(Final_Weight = w,
Alpha = alpha,
All_pars = allPars,
Training_error = training_error,
Test_error = test_error
))
}
B = 1000
implementation = F
if (implementation){
run = Adaboost(train_matrix, test_matrix, B = B, show_status = T)
save(run, file = "../hw 5/Implementation_results_1000.Rdata")
}
load(paste0(path, "/Implementation_results_1000.Rdata"))
library(ggplot2)
library(reshape)
error_data = data.frame(Iteration = c(1:B), Train_error = run$Training_error, Test_error = run$Test_error)
error_melted = melt(error_data, id=c('Iteration'))
ggplot(data = error_melted)+
geom_line(mapping = aes(x = Iteration, y = value, color = factor(variable)))+
labs(title = "AdaBoost Errors", x = "b", y = "Error")
save(tesseract_vec, file = paste0(project_path,"/output/TesseractTxt.Rdata"))
## Here, we compare the lower case version of the tokens
old_intersect_vec <- vecsets::vintersect(tolower(ground_truth_vec), tolower(tesseract_vec)) #607
length(old_intersect_vec)/length(ground_truth_vec)
#setwd("~/Desktop/GR5243 Applied Data Science/Project 4/project 4 edit")
project_path = "~/Desktop/GR5243 Applied Data Science/Project 4/Spring2019-Proj4-grp8-1"
true_path = "/data/ground_truth/"
OCR_path = "/data/tesseract/"
#true_path <- "../data/ground_truth/"
#OCR_path <- "../data/tesseract/"
#correctPath = "../output/correct/"
ground_truth = list.files(path=paste0(project_path,true_path), pattern="*.txt", full.names=TRUE, recursive=FALSE)
OCR = list.files(path=paste0(project_path,OCR_path), pattern="*.txt", full.names=TRUE, recursive=FALSE)
#if(length(truthFiles) != length(ocrFiles)) stop("the number of ground truth files is not equal to the number of OCR recognized files")
n = length(ground_truth) # number of files
# select train + validation and testing data 80% and 20%
#set.seed(2019)
#train = sample(1:n, size = 0.7*n, replace = F)
# set training data
#truthTrain = ground_truth[train]
#ocrTrain = OCR[train]
# set testing data
#truthTest = ground_truth[-train]
#ocrTest = OCR[-train]
filename = ocrTrain[1]  # for debugging
filename = OCR[1]  # for debugging
#text = paste(readLines(filename), collapse = " ")
text = readLines(filename)
text<-gsub("[[:punct:]]", " ", text)
text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
text[1:10]
word = strsplit(text," ")[[1]]
word[[1]]
word
word = strsplit(text," ")
word
length(text)
length(word)
word_list = word[word!=""]
length(word_list)
words = word[word!=""]
corpus = VCorpus(VectorSource(words))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeWords, character(0))%>%
tm_map(removeNumbers)%>%
tm_map(removePunctuation)
ocr_words = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
length(ocr_words)
ocr_words
lines = word[word!=""]
cleaned_doc = list()
cleaned_doc = list()
lines = word[word!=""]
for (line in 1:length(lines)){
uncleaned = words[[line]]
corpus = VCorpus(VectorSource(uncleaned))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)%>%
cleaned = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
cleaned = as.vector(cleaned)
cleaned = cleaned[nchar(cleaned) > 1] # no single character words
cleaned = cleaned[nchar(cleaned) <= 24] # no words longer than the longest word in     ground truth
cleaned_doc[[line]] = cleaned
}
library(stringr)
library(tm)
library(tidytext)
library(dplyr)
for (line in 1:length(lines)){
uncleaned = words[[line]]
corpus = VCorpus(VectorSource(uncleaned))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)%>%
cleaned = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
cleaned = as.vector(cleaned)
cleaned = cleaned[nchar(cleaned) > 1] # no single character words
cleaned = cleaned[nchar(cleaned) <= 24] # no words longer than the longest word in     ground truth
cleaned_doc[[line]] = cleaned
}
line = 1
uncleaned = lines[[line]]
corpus = VCorpus(VectorSource(uncleaned))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)%>%
cleaned = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
corpus = VCorpus(VectorSource(uncleaned))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)
cleaned = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
cleaned = as.vector(cleaned)
cleaned = cleaned[nchar(cleaned) > 1] # no single character words
cleaned = cleaned[nchar(cleaned) <= 24] # no words longer than the longest word in     ground truth
cleaned
cleaned
cleaned = as.vector(cleaned)
cleaned
cleaned = as.matrix(cleaned)
cleaned
cleaned_doc[[line]] = cleaned
cleaned_doc[[line]]
cleaned_doc = list()
for (line in 1:length(lines)){
line = 1
uncleaned = lines[[line]]
corpus = VCorpus(VectorSource(uncleaned))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)
cleaned = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
cleaned = as.matrix(cleaned)
cleaned = cleaned[nchar(cleaned) > 1] # no single character words
cleaned = cleaned[nchar(cleaned) <= 24] # no words longer than the longest word in     ground truth
cleaned_doc[[line]] = cleaned
}
length(cleaned_doc)
for (line in 1:length(lines)){
#line = 1
uncleaned = lines[[line]]
corpus = VCorpus(VectorSource(uncleaned))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)
cleaned = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
cleaned = as.matrix(cleaned)
cleaned = cleaned[nchar(cleaned) > 1] # no single character words
cleaned = cleaned[nchar(cleaned) <= 24] # no words longer than the longest word in     ground truth
cleaned_doc[[line]] = cleaned
}
length(cleaned_doc)
check_punct = grep("[[:punct:]]", cleaned_doc)
check_punct
cleaned_doc
#text = paste(readLines(filename), collapse = " ")
text = readLines(filename)
#text<-gsub("[[:punct:]]", " ", text)
text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
word = strsplit(text," ")
lines = word[word!=""]
cleaned_doc = list()
for (line in 1:length(lines)){
#line = 1
uncleaned = lines[[line]]
corpus = VCorpus(VectorSource(uncleaned))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)%>%
tm_map(removePunctuation)
cleaned = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
cleaned = as.matrix(cleaned)
cleaned = cleaned[nchar(cleaned) > 1] # no single character words
cleaned = cleaned[nchar(cleaned) <= 24] # no words longer than the longest word in     ground truth
cleaned_doc[[line]] = cleaned
}
cleaned_doc
OCR_Extract <- function(filename){   # for extract single ocr .txt only
#filename = OCR[1]  # for debugging
#text = ""
#for (file in filenames){
#file = filenames[1]
#text_lines = readLines(file)
# text = paste(text,
#              paste(text_lines, collapse = " ")
#             )
#}
#text = paste(readLines(filename), collapse = " ")
text = readLines(filename)
#text<-gsub("[[:punct:]]", " ", text)
text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
word = strsplit(text," ")
lines = word[word!=""]
cleaned_doc = list()
for (line in 1:length(lines)){
#line = 1
uncleaned = lines[[line]]
corpus = VCorpus(VectorSource(uncleaned))%>%
tm_map(stripWhitespace)%>%
tm_map(content_transformer(tolower))%>%
tm_map(removeNumbers)%>%
tm_map(removePunctuation)
cleaned = tidy(corpus) %>%
select(text) %>%
unnest_tokens(dictionary, text)
cleaned = as.matrix(cleaned)
cleaned = cleaned[nchar(cleaned) > 1] # no single character words
cleaned = cleaned[nchar(cleaned) <= 24] # no words longer than the longest word in     ground truth
cleaned_doc[[line]] = cleaned
}
return(cleaned_doc)
}
load(paste0(project_path,"/output/digrams.Rdata"))
f = 25  # for debugging
#filenames = ocrTrain[1:5]   # for debugging
filenames = OCR  # for debugging
Detection_list = list()
doc = OCR_Extract(filename = filenames[f])
l = 1
nwords = length(doc[[l]]) # number of words in current line
nwords
doc[[l]]
doc_list = list()
l = 1   # for debugging
nwords = length(doc[[l]]) # number of words in current line
error_det = matrix(NA, nrow = nwords, ncol = 2)
error_det[,1] = doc[[l]]  # fill in exact words
error_det[,2] = 0  # initial values, in character mode
for (w in 1:nwords){
count_pair = 1
stop = F
word = error_det[w,1]
n = nchar(word)
for (i in 1:(n-1)){
if (stop) break # stop looping current word (i loop)
for(j in 2:n){
if (i<j){
row_index = match(substr(word, i, i),letters)
col_index = match(substr(word, j, j),letters)
if(digrams[[n]][[count_pair]][row_index,col_index] == 0){
error_det[w,2] = 1
stop = T
break  # stop j loop
}
count_pair = count_pair + 1
}
}
}
}
load(paste0(project_path,"/output/digrams.Rdata"))
for (w in 1:nwords){
count_pair = 1
stop = F
word = error_det[w,1]
n = nchar(word)
for (i in 1:(n-1)){
if (stop) break # stop looping current word (i loop)
for(j in 2:n){
if (i<j){
row_index = match(substr(word, i, i),letters)
col_index = match(substr(word, j, j),letters)
if(dic_digrams[[n]][[count_pair]][row_index,col_index] == 0){
error_det[w,2] = 1
stop = T
break  # stop j loop
}
count_pair = count_pair + 1
}
}
}
}
error_det
doc_list[[l]] = error_det
doc_list[[l]]
Detection_list = list()
filenames = OCR[1:5]   # for debugging
filenames = OCR[1:3]   # for debugging
for (f in 1:length(filenames)){
#f = 25  # for debugging
doc_list = list()
doc = OCR_Extract(filename = filenames[f])
L = length(doc)  # number of lines in current document
for (l in 1:L){  # counting lines in current document
#l = 1   # for debugging
nwords = length(doc[[l]]) # number of words in current line
error_det = matrix(NA, nrow = nwords, ncol = 2)
error_det[,1] = doc[[l]]  # fill in exact words
error_det[,2] = 0  # initial values, in character mode
#######  Start detection  ######
for (w in 1:nwords){
count_pair = 1
stop = F
word = error_det[w,1]
n = nchar(word)
for (i in 1:(n-1)){
if (stop) break # stop looping current word (i loop)
for(j in 2:n){
if (i<j){
row_index = match(substr(word, i, i),letters)
col_index = match(substr(word, j, j),letters)
if(dic_digrams[[n]][[count_pair]][row_index,col_index] == 0){
error_det[w,2] = 1
stop = T
break  # stop j loop
}
count_pair = count_pair + 1
}
}
}
}
#######  End detection  ######
doc_list[[l]] = error_det
}
cat("OCR",f,"Done! \n")
Detection_list[[f]] = doc_list
}
cleaned_doc
length(cleaned_doc[[47]])
for (l in 1:L){  # counting lines in current document
#l = 1   # for debugging
nwords = length(doc[[l]]) # number of words in current line
if (nwords == 0) next
error_det = matrix(NA, nrow = nwords, ncol = 2)
error_det[,1] = doc[[l]]  # fill in exact words
error_det[,2] = 0  # initial values, in character mode
#######  Start detection  ######
for (w in 1:nwords){
count_pair = 1
stop = F
word = error_det[w,1]
n = nchar(word)
for (i in 1:(n-1)){
if (stop) break # stop looping current word (i loop)
for(j in 2:n){
if (i<j){
row_index = match(substr(word, i, i),letters)
col_index = match(substr(word, j, j),letters)
if(dic_digrams[[n]][[count_pair]][row_index,col_index] == 0){
error_det[w,2] = 1
stop = T
break  # stop j loop
}
count_pair = count_pair + 1
}
}
}
}
#######  End detection  ######
doc_list[[l]] = error_det
}
Detection_list = list()
filenames = OCR[1:3]   # for debugging
for (f in 1:length(filenames)){
#f = 25  # for debugging
doc_list = list()
doc = OCR_Extract(filename = filenames[f])
L = length(doc)  # number of lines in current document
for (l in 1:L){  # counting lines in current document
#l = 1   # for debugging
nwords = length(doc[[l]]) # number of words in current line
if (nwords == 0) next
error_det = matrix(NA, nrow = nwords, ncol = 2)
error_det[,1] = doc[[l]]  # fill in exact words
error_det[,2] = 0  # initial values, in character mode
#######  Start detection  ######
for (w in 1:nwords){
count_pair = 1
stop = F
word = error_det[w,1]
n = nchar(word)
for (i in 1:(n-1)){
if (stop) break # stop looping current word (i loop)
for(j in 2:n){
if (i<j){
row_index = match(substr(word, i, i),letters)
col_index = match(substr(word, j, j),letters)
if(dic_digrams[[n]][[count_pair]][row_index,col_index] == 0){
error_det[w,2] = 1
stop = T
break  # stop j loop
}
count_pair = count_pair + 1
}
}
}
}
#######  End detection  ######
doc_list[[l]] = error_det
}
cat("OCR",f,"Done! \n")
Detection_list[[f]] = doc_list
}
length(Detection_list)
length(Detection_list[[1]])
Detection_list[[1]][[47]]
Detection_list[[1]][[46]]
cleaned_doc[[46]]
Detection_list[[1]][[45]]
Detection_list[[1]][[2]]
Detection_list[[1]][[4]]
Detection_list[[1]][[44]]
cleaned_doc[[2]]
cleaned_doc[[4]]
