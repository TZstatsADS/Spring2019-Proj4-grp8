---
title: 'Detection Part'
output: html_notebook
---

Xinyi Hu

```{r Load libraries}

library(stringr)
library(tm)
library(tidytext)
library(dplyr)

```

```{r read files}
#setwd("~/Desktop/GR5243 Applied Data Science/Project 4/project 4 edit")

project_path = "~/Desktop/GR5243 Applied Data Science/Project 4/Spring2019-Proj4-grp8-1"
true_path = "/data/ground_truth/"
OCR_path = "/data/tesseract/"

#true_path <- "../data/ground_truth/"
#OCR_path <- "../data/tesseract/"
#correctPath = "../output/correct/"

ground_truth = list.files(path=paste0(project_path,true_path), pattern="*.txt", full.names=TRUE, recursive=FALSE)
OCR = list.files(path=paste0(project_path,OCR_path), pattern="*.txt", full.names=TRUE, recursive=FALSE)

#if(length(truthFiles) != length(ocrFiles)) stop("the number of ground truth files is not equal to the number of OCR recognized files")

n = length(ground_truth) # number of files

# select train + validation and testing data 80% and 20%
set.seed(2019)
train = sample(1:n, size = 0.7*n, replace = F)

# set training data
truthTrain = ground_truth[train]
ocrTrain = OCR[train]

# set testing data
truthTest = ground_truth[-train]
ocrTest = OCR[-train]
```

```{r Extract Ground Truth}

Truth_Extract = function(filenames){

  #filenames <- ground_truth[1:5]  # for debugging
  
  text = ""
  for (file in filenames){
    #file = filenames[1]
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
  }
  
  #text = "applied data science is challenging.!sfsfas 09073 \\W"
  text<-gsub("[[:punct:]]", " ", text)
  text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
  word = strsplit(text," ")[[1]]
  words = word[word!=""]

  corpus = VCorpus(VectorSource(words))%>%
    tm_map(stripWhitespace)%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removeWords, character(0))%>%
    tm_map(removeNumbers)%>%
    tm_map(removePunctuation)

  truth_words = tidy(corpus) %>%
    select(text) %>%
    unnest_tokens(dictionary, text)
    
  
  truth_words = unique(truth_words)
  truth_words = as.matrix(truth_words)
  truth_words = truth_words[nchar(truth_words) > 1] # no single character words
  #truth_words = truth_words[truth_words!=""]

  #check_punct = grep("[[:punct:]]", truth_words)
  #check_num = grep("[0-9]", truth_words)
  #check_punct
  #check_num

  
  return(truth_words)
}

#dictionary = Truth_Extract(ground_truth)

#save(dictionary, file = paste0(project_path,"/output/GroundTruthWords.Rdata"))
```

```{r Digrams short words}
short = "abc ab ba ac bac cba cca bba aab"
string = strsplit(short,split = " ")[[1]];string
N = max(nchar(string))
dic = list()

for (i in 2:N){     # empty dictionary digrams
   
  dic[[i]] = list()   # word length
  n = i*(i-1)/2      # number of distinct pairs in word with length i
  
  for (j in 1:n){
    
    dic[[i]][[j]] = matrix(0, nrow = 4, ncol = 4)
    
  }
}
dic[[1]] = "No single-character words"


for (word in string){
  
  n = nchar(word)
  count_pair = 1
  
  for (i in 1:(n-1)){
    for (j in 2:n){
      if (i<j){
        
        row_index = match(substr(word, i, i),letters)
        col_index = match(substr(word, j, j),letters)
        dic[[n]][[count_pair]][row_index,col_index] = 1
        count_pair = count_pair + 1
      }
    }
  }
}
dic[[2]]
dic[[3]]
```

```{r Digrams 36x36 26x26}
#text = "applied data science is challenging"
#string = strsplit(text,split = " ")[[1]];string
#N = max(nchar(string))

test_dic = dictionary[1:1000]
N = max(nchar(test_dic))
dic = list()
#numberletters = c(0:9, letters)

for (i in 2:N){     # empty dictionary digrams
  dic[[i]] = list()   # word length
  #cat(i, "\n")
  n = i*(i-1)/2      # number of distinct pairs in word with length i
  for (j in 1:n){
    #dic[[i]][[j]] = matrix(0, nrow = 36, ncol = 36)
    dic[[i]][[j]] = matrix(0, nrow = 26, ncol = 26)

  }
}
dic[[1]] = "No single-character words"
length(dic[[4]])   # Check
length(dic[[5]])

#for (word in string){
for (word in test_dic){
  
  n = nchar(word)
  count_pair = 1
  
  for (i in 1:(n-1)){
    
    for (j in 2:n){
      
      if (i<j){
        
        #row_index = match(substr(word, i, i),numberletters)
        #col_index = match(substr(word, j, j),numberletters)
        row_index = match(substr(word, i, i),letters)
        col_index = match(substr(word, j, j),letters)
        dic[[n]][[count_pair]][row_index,col_index] = 1
        count_pair = count_pair + 1
        
      }
    }
  }
}
#check
dic[[1]]
apply(dic[[5]][[4]],2,sum)
```

```{r Dic-Digrams}

load(paste0(project_path,"/output/GroundTruthWords.Rdata"))

Digrams = function(dictionary){
  
  dic = dictionary  # character vector
  
  N = max(nchar(dic))
  digrams = list()
  #numberletters = c(0:9, letters)

  for (i in 2:N){     # empty dictionary digrams
    
    digrams[[i]] = list()   # word length
    n = i*(i-1)/2      # number of distinct pairs in word with length i
    
    for (j in 1:n){
      #dic[[i]][[j]] = matrix(0, nrow = 36, ncol = 36)
      digrams[[i]][[j]] = matrix(0, nrow = 26, ncol = 26)
  
    }
  }
  digrams[[1]] = "No single-character words"
  #length(digrams) == N

  #for (word in string){
  for (word in dic){
  
    n = nchar(word)
    count_pair = 1
  
    for (i in 1:(n-1)){
    
      for (j in 2:n){
      
        if (i<j){
        
          #row_index = match(substr(word, i, i),numberletters)
          #col_index = match(substr(word, j, j),numberletters)
          row_index = match(substr(word, i, i),letters)
          col_index = match(substr(word, j, j),letters)
          digrams[[n]][[count_pair]][row_index,col_index] = 1
          count_pair = count_pair + 1
        
        }
      }
    }
  }

  # apply(digrams[[24]][[270]],2,sum)
  # length(digrams[[24]])

  
  return(digrams)
}

#dic_digrams = Digrams(dictionary)

#save(dic_digrams, file = paste0(project_path,"/output/digrams.Rdata"))

```


```{r Extract OCR version 1 Failed}
  
  filenames = ocrTrain[1:5]
  text = ""
  for (file in filenames){
    
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
  }
  word = strsplit(text," ")[[1]]
  words = word[word!=""]
  n = length(words)
  
  
  ocr_words = c()
  i = 1            
  for (file in filenames){
    #file = filenames[1]
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
    #txt_word = unlist(strsplit(text," "))
    ocr_words[i] = text
    i = i+1               # count .txt 
  }
  
  ocr_txt_word = strsplit(ocr_words," ")   
  # each of 100 txts contains its own words, store in a single list
  
  for (i in 1:length(ocr_txt_word)){
    ocr_txt_word[[i]] = ocr_txt_word[[i]][ocr_txt_word[[i]]!=""]# clean null words
  }
  
  n1 = nchar(ocr_txt_word[[1]])
  sum(n1)
  
  n = sum(nchar(ocr_txt_word))
  
  word_map = matrix(NA, ncol = 4, nrow = n)
  colnames(word_map) = c("word", "word_index", "txt", "incorrect")
  
  i = 1
  for (t in 1:length(ocr_txt_word)){
  # t = 1
    for (w in 1:length(ocr_txt_word[[t]])){
   #   w = 2
      word_map[i,] = c(ocr_txt_word[[t]][w], w, t, 0)
      i = i+1
    }
  }
  see = as.data.frame(word_map)
  na.index = is.na(word_map[,1])
  ####################################################
  # create dataset of words

  # tokens -> list[[docs]] -> each [[doc]] -> [[lines]] -> each [[line]] -> [words]
  
  bag <- matrix(NA, ncol = 5, nrow = n, 
                dimnames = list(rep("", n), c("token", "word", "line", "doc", "error")))
  
  i <- 1
  for(d in 1:length(tokens)){ # each txt
    for(l in 1:length(tokens[[d]])){ # d th txt with n_d words
      for(w in 1:length(tokens[[d]][[l]])){ # extract each word in d th txt
        bag[i,] <- c(tokens[[d]][[l]][w], w, l, d, 0)  # c(exact word, word posi in line l, line l in doc d, doc d in tokens, error)
        i <- i + 1
      }
    }
  }
  
  #d = 1;l = 3
  #head(tokens[[d]])
  #tokens[[d]][[l]]
  #length(tokens[[d]][[l]])
  
  bag <- bag[complete.cases(bag),]
  length(bag)
  nrow(bag)
  
  # toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
  
  corpus <- VCorpus(VectorSource(bag[,1]))%>%
    tm_map(content_transformer(tolower))%>%
    # tm_map(toSpace, "\\W")%>%
    tm_map(removePunctuation)%>%
    tm_map(removeWords, character(0))%>%
    tm_map(stripWhitespace)
  
  library(textreg)
  b <- convert.tm.to.character(corpus)
  bag[,1] <- b
  
  assign(outName, bag)
  save(list=outName, file = paste0("../output/", outName, ".RData"))
  return(eval(as.name(paste(outName))))

```


```{r Extract OCR version 2 Failed}
  
  filenames = ocrTrain[1:5]
  #text = ""
  text = list()
  i = 1
  for (file in filenames){
    
    text[[i]] = readLines(file)
    #text_lines = readLines(file)
   # text = paste(text, 
    #              paste(text_lines, collapse = " ")
     #             )
    i = i+1
  }
  word = strsplit(text," ")[[1]]
  words = word[word!=""]
  n = length(words)
  
  
  ocr_words = c()
  i = 1            
  for (file in filenames){
    #file = filenames[1]
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
    #txt_word = unlist(strsplit(text," "))
    ocr_words[i] = text
    i = i+1               # count .txt 
  }
  
  ocr_txt_word = strsplit(ocr_words," ")   
  # each of 100 txts contains its own words, store in a single list
  
  for (i in 1:length(ocr_txt_word)){
    ocr_txt_word[[i]] = ocr_txt_word[[i]][ocr_txt_word[[i]]!=""]# clean null words
  }
  
  n1 = nchar(ocr_txt_word[[1]])
  sum(n1)
  
  n = sum(nchar(ocr_txt_word))
  
  word_map = matrix(NA, ncol = 4, nrow = n)
  colnames(word_map) = c("word", "word_index", "txt", "incorrect")
  
  i = 1
  for (t in 1:length(ocr_txt_word)){
  # t = 1
    for (w in 1:length(ocr_txt_word[[t]])){
   #   w = 2
      word_map[i,] = c(ocr_txt_word[[t]][w], w, t, 0)
      i = i+1
    }
  }
  see = as.data.frame(word_map)
  na.index = is.na(word_map[,1])
  ####################################################
  # create dataset of words

  # tokens -> list[[docs]] -> each [[doc]] -> [[lines]] -> each [[line]] -> [words]
  
  bag <- matrix(NA, ncol = 5, nrow = n, 
                dimnames = list(rep("", n), c("token", "word", "line", "doc", "error")))
  
  i <- 1
  for(d in 1:length(tokens)){ # each txt
    for(l in 1:length(tokens[[d]])){ # d th txt with n_d words
      for(w in 1:length(tokens[[d]][[l]])){ # extract each word in d th txt
        bag[i,] <- c(tokens[[d]][[l]][w], w, l, d, 0)  # c(exact word, word posi in line l, line l in doc d, doc d in tokens, error)
        i <- i + 1
      }
    }
  }
  
  #d = 1;l = 3
  #head(tokens[[d]])
  #tokens[[d]][[l]]
  #length(tokens[[d]][[l]])
  
  bag <- bag[complete.cases(bag),]
  length(bag)
  nrow(bag)
  
  # toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
  
  corpus <- VCorpus(VectorSource(bag[,1]))%>%
    tm_map(content_transformer(tolower))%>%
    # tm_map(toSpace, "\\W")%>%
    tm_map(removePunctuation)%>%
    tm_map(removeWords, character(0))%>%
    tm_map(stripWhitespace)
  
  library(textreg)
  b <- convert.tm.to.character(corpus)
  bag[,1] <- b
  
  assign(outName, bag)
  save(list=outName, file = paste0("../output/", outName, ".RData"))
  return(eval(as.name(paste(outName))))

```


```{r Extract OCR version 3}

for(i in 1:length(ocrTrain)){  
  
  i = 1   # for debugging
  
  words = OCR_Extract(ocrTrain[i])  
  
  #textraw=Truth_Extract(ocrTest[3]) #extract ith file's content
 # save(textraw, file = paste0(project_path,"/output/textraw.Rdata"))
  #stringraw=strsplit(textraw,split = " ")[[1]]
  #stringprocess1 <-  stringraw[ stringraw!=""] # 去掉空格元素
 # corpus <- VCorpus(VectorSource(stringprocess1))%>%
   # tm_map(stripWhitespace)%>%
   # tm_map(content_transformer(tolower))%>%
   # tm_map(removeWords, character(0))%>%
   # tm_map(removeNumbers)%>%
   # tm_map(removePunctuation)

   # stringprocess2 <- tidy(corpus) %>%
   # select( stringraw) %>%
   # unnest_tokens(textraw,  stringraw)
    # remove space and mark and change to lower
    
    error_det_loc <- data.frame()
    
    k=0

    for( j in 1:length(words)){
      j = 2  #"asslstance"
      n <- nchar(words[j])
    
      count_pair = 0
   
       for(l in 1:(n-1)){ 
         
         for (m in 2:n) { 
           
            if (l<m){
              
              count_pair <- count_pair+1
              row_index = match(substr(words[j], l, l),letters)
              col_index = match(substr(words[j], m, m),letters)
              
             if(digrams[[n]][[count_pair]][row_index,col_index] == 0){
                k=k+1
                error_det_loc[k,]<-c(i,j) # j th word in i th ocr .txt is incorrect
                break #
        }
    }
  
         } 
         }
    }   
}

tail(text)
 
```

```{r OCR_Extract}
OCR_Extract <- function(filename){   # for extract single ocr .txt only 

  #filename = ocrTrain[1]  # for debugging
  #text = ""
  #for (file in filenames){
    #file = filenames[1]
    #text_lines = readLines(file)
   # text = paste(text, 
    #              paste(text_lines, collapse = " ")
     #             )
  #}
  
  text = paste(readLines(filename), collapse = " ")
  text<-gsub("[[:punct:]]", " ", text)
  text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
  word = strsplit(text," ")[[1]]
  words = word[word!=""]

  corpus = VCorpus(VectorSource(words))%>%
    tm_map(stripWhitespace)%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removeWords, character(0))%>%
    tm_map(removeNumbers)%>%
    tm_map(removePunctuation)

  ocr_words = tidy(corpus) %>%
    select(text) %>%
    unnest_tokens(dictionary, text)
    
  
  ocr_words = as.matrix(ocr_words)
  ocr_words = ocr_words[nchar(ocr_words) > 1] # no single character words
  ocr_words = ocr_words[nchar(ocr_words) <= 24] # no words longer than the longest word in ground truth

  
  return(ocr_words)
}
```



```{r Extract OCR version 4}
load(paste0(project_path,"/output/digrams.Rdata"))

Detection = function(filenames, digrams){
  
  Detection_list = list()
  #filenames = ocrTrain[1:5]   # for debugging
  #filenames = ocrTrain  # for debugging
  #digrams = dic_digrams  # for debugging

    
  for (f in 1:length(filenames)){
    
    #f = 25  # for debugging
    words = OCR_Extract(filename = filenames[f])
    N = length(words)
    
    error_det = matrix(NA, nrow = N, ncol = 2)
    error_det[,1] = words
    error_det[,2] = 0   # Initial values, character mode! 
    
    #######  Start detection  ######
    for (w in 1:N){
     # w = 3498  # for debugging
      
      count_pair = 1
      word = error_det[w,1]
      n = nchar(word)
      
      for (i in 1:(n-1)){
        
        for(j in 2:n){
          
          if (i<j){
            
              row_index = match(substr(word, i, i),letters)
              col_index = match(substr(word, j, j),letters)

              if(digrams[[n]][[count_pair]][row_index,col_index] == 0){
                error_det[w,2] = 1
                break
              }
              
              count_pair = count_pair + 1

          }
          
        }
        
    
      }
      

    }
    
    cat("OCR",f,"Done! \n")
    Detection_list[[f]] = error_det

 
  }
  
  return(Detection_list)
  
}



detection_train = Detection(ocrTrain, digrams = dic_digrams)

save(detection_train, file = paste0(project_path,"/output/detection_train.Rdata"))



astr <- "Ábcdêãçoàúü"
astr = c("lmageﬁaulldlng","avsfeåååååå")
iconv(astr, from = 'UTF-8', to = 'ASCII//TRANSLIT')

```



