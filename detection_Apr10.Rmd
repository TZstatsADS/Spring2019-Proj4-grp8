---
title: 'Detection Part'
output: html_notebook
---

Xinyi Hu

```{r Load libraries}

library(stringr)
library(tm)
library(tidytext)
library(dplyr)

```

```{r read files}
#setwd("~/Desktop/GR5243 Applied Data Science/Project 4/project 4 edit")

project_path = "~/Desktop/GR5243 Applied Data Science/Project 4/Spring2019-Proj4-grp8-1"
true_path = "/data/ground_truth/"
OCR_path = "/data/tesseract/"

#true_path <- "../data/ground_truth/"
#OCR_path <- "../data/tesseract/"
#correctPath = "../output/correct/"

ground_truth = list.files(path=paste0(project_path,true_path), pattern="*.txt", full.names=TRUE, recursive=FALSE)
OCR = list.files(path=paste0(project_path,OCR_path), pattern="*.txt", full.names=TRUE, recursive=FALSE)

#if(length(truthFiles) != length(ocrFiles)) stop("the number of ground truth files is not equal to the number of OCR recognized files")

n = length(ground_truth) # number of files

```

```{r Extract Ground Truth}

Truth_Extract = function(filenames){

  #filenames <- ground_truth  # for debugging
  
  text = ""
  for (file in filenames){
    #file = filenames[1]
    text_lines = readLines(file)
    text = paste(text, 
                  paste(text_lines, collapse = " ")
                  )
  }
  
  #text = "app lied data science is challenging.!sfsfas 09073 \\W"
  #text<-gsub("[[:punct:]]", " ", text)
  text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
  word = strsplit(text," ")[[1]]
  words = word[word!=""]

  corpus = VCorpus(VectorSource(words))%>%
    tm_map(stripWhitespace)%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removeWords, character(0))%>%
    tm_map(removeNumbers)%>%
    tm_map(removePunctuation)

  
  truth_words = tidy(corpus) %>%
    select(text) %>%
    unnest_tokens(dictionary, text)
    
  
  truth_words = unique(truth_words)
  truth_words = as.matrix(truth_words)
  truth_words = truth_words[nchar(truth_words) > 1] # no single character words
  #truth_words = truth_words[truth_words!=""]

  #check_punct = grep("[[:punct:]]", truth_words)
  #check_num = grep("[0-9]", truth_words)
  #check_punct
  #check_num

  
  return(truth_words)
}

#dictionary = Truth_Extract(ground_truth)

#save(dictionary, file = paste0(project_path,"/output/GroundTruthWords.Rdata"))
```


```{r Dic-Digrams}

load(paste0(project_path,"/output/GroundTruthWords.Rdata"))

Digrams = function(dictionary){
  
  dic = dictionary  # character vector
  
  N = max(nchar(dic))
  digrams = list()
  #numberletters = c(0:9, letters)

  for (i in 2:N){     # empty dictionary digrams
    
    digrams[[i]] = list()   # word length
    n = i*(i-1)/2      # number of distinct pairs in word with length i
    
    for (j in 1:n){
      #dic[[i]][[j]] = matrix(0, nrow = 36, ncol = 36)
      digrams[[i]][[j]] = matrix(0, nrow = 26, ncol = 26)
  
    }
  }
  digrams[[1]] = "No single-character words"
  #length(digrams) == N

  #for (word in string){
  for (word in dic){
  
    n = nchar(word)
    count_pair = 1
  
    for (i in 1:(n-1)){
    
      for (j in 2:n){
      
        if (i<j){
        
          #row_index = match(substr(word, i, i),numberletters)
          #col_index = match(substr(word, j, j),numberletters)
          row_index = match(substr(word, i, i),letters)
          col_index = match(substr(word, j, j),letters)
          digrams[[n]][[count_pair]][row_index,col_index] = 1
          count_pair = count_pair + 1
        
        }
      }
    }
  }

  # apply(digrams[[24]][[270]],2,sum)
  # length(digrams[[24]])

  
  return(digrams)
}

#dic_digrams = Digrams(dictionary)

#save(dic_digrams, file = paste0(project_path,"/output/digrams.Rdata"))

write_csv(as.data.frame(dictionary), path = paste0(project_path,"/output/dictionary.csv"))

head(as.data.frame(dictionary))
mode(dictionary)
```


```{r OCR_Extract}
OCR_Extract <- function(filename){   # for extract single ocr .txt only 

  #filename = ocrTrain[1]  # for debugging
  #text = ""
  #for (file in filenames){
    #file = filenames[1]
    #text_lines = readLines(file)
   # text = paste(text, 
    #              paste(text_lines, collapse = " ")
     #             )
  #}
  
  text = paste(readLines(filename), collapse = " ")
  text<-gsub("[[:punct:]]", " ", text)
  text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
  word = strsplit(text," ")[[1]]
  words = word[word!=""]

  corpus = VCorpus(VectorSource(words))%>%
    tm_map(stripWhitespace)%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removeWords, character(0))%>%
    tm_map(removeNumbers)%>%
    tm_map(removePunctuation)

  ocr_words = tidy(corpus) %>%
    select(text) %>%
    unnest_tokens(dictionary, text)
    
  
  ocr_words = as.matrix(ocr_words)
  ocr_words = ocr_words[nchar(ocr_words) > 1] # no single character words
  ocr_words = ocr_words[nchar(ocr_words) <= 24] # no words longer than the longest word in ground truth

  
  return(ocr_words)
}
```


```{r Extract OCR version 3}
load(paste0(project_path,"/output/digrams.Rdata"))

Detection = function(filenames, digrams){
  
  Detection_list = list()
  #filenames = ocrTrain[1:5]   # for debugging
  #filenames = ocrTrain  # for debugging
  #digrams = dic_digrams  # for debugging

    
  for (f in 1:length(filenames)){
    
    #f = 25  # for debugging
    words = OCR_Extract(filename = filenames[f])
    N = length(words)
    
    error_det = matrix(NA, nrow = N, ncol = 2)
    error_det[,1] = words
    error_det[,2] = 0   # Initial values, character mode! 
    
    #######  Start detection  ######
    for (w in 1:N){
     # w = 3498  # for debugging
      
      count_pair = 1
      word = error_det[w,1]
      n = nchar(word)
      
      for (i in 1:(n-1)){
        
        for(j in 2:n){
          
          if (i<j){
            
              row_index = match(substr(word, i, i),letters)
              col_index = match(substr(word, j, j),letters)

              if(digrams[[n]][[count_pair]][row_index,col_index] == 0){
                error_det[w,2] = 1
                break
              }
              
              count_pair = count_pair + 1

          }
          
        }
        
    
      }
      

    }
    
    cat("OCR",f,"Done! \n")
    Detection_list[[f]] = error_det

 
  }
  
  return(Detection_list)
  
}


#detection_ocr = Detection(OCR, digrams = dic_digrams)

#save(detection_ocr, file = paste0(project_path,"/output/detection_ocr.Rdata"))

load(paste0(project_path,"/output/detection_ocr.Rdata"))
# Check
length(detection_ocr)
head(detection_ocr[[1]],20)
tail(detection_ocr[[70]],20)

###### debug ######
astr <- "Ábcdêãçoàúü"
astr = c("lmageﬁaulldlng","avsfeåååååå")
iconv(astr, from = 'UTF-8', to = 'ASCII//TRANSLIT')

```


```{r OCR_Extract version 2}
OCR_Extract <- function(filename){   # for extract single ocr .txt only 

  #filename = OCR[1]  # for debugging

  #text = paste(readLines(filename), collapse = " ")
  text = readLines(filename)
  #text<-gsub("[[:punct:]]", " ", text)
  text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
  
  word = strsplit(text," ")
  lines = word[word!=""]
  
  cleaned_doc = list()
  
  for (line in 1:length(lines)){
    #line = 1
    uncleaned = lines[[line]]
    
    corpus = VCorpus(VectorSource(uncleaned))%>%
      tm_map(stripWhitespace)%>%
      tm_map(content_transformer(tolower))%>%
      tm_map(removeNumbers)%>%
      tm_map(removePunctuation)

    cleaned = tidy(corpus) %>%
      select(text) %>%
      unnest_tokens(dictionary, text)

    cleaned = as.matrix(cleaned)
    cleaned = cleaned[nchar(cleaned) > 1] # no single character words
    cleaned = cleaned[nchar(cleaned) <= 24] # no words longer than the longest word in     ground truth

    cleaned_doc[[line]] = cleaned
    
  }

  return(cleaned_doc)
}
```


```{r Extract OCR version 4}
load(paste0(project_path,"/output/digrams.Rdata"))

Detection = function(filenames, digrams){
  
  Detection_list = list()
  filenames = OCR[1:3]   # for debugging
  #filenames = OCR  # for debugging
  #digrams = dic_digrams  # for debugging

  for (f in 1:length(filenames)){
    
    #f = 25  # for debugging
    doc_list = list()
    
    doc = OCR_Extract(filename = filenames[f])
    L = length(doc)  # number of lines in current document
    
    for (l in 1:L){  # counting lines in current document
      
      #l = 1   # for debugging
      nwords = length(doc[[l]]) # number of words in current line
      if (nwords == 0) next  # no words in current line
      
      error_det = matrix(NA, nrow = nwords, ncol = 2)
      error_det[,1] = doc[[l]]  # fill in exact words
      error_det[,2] = 0  # initial values, in character mode
      
      #######  Start detection  ######
      
      for (w in 1:nwords){
        
        count_pair = 1
        stop = F
        word = error_det[w,1]
        n = nchar(word)
      
        for (i in 1:(n-1)){
          
          if (stop) break # stop looping current word (i loop)
        
          for(j in 2:n){
          
            if (i<j){
            
                row_index = match(substr(word, i, i),letters)
                col_index = match(substr(word, j, j),letters)

                if(dic_digrams[[n]][[count_pair]][row_index,col_index] == 0){
                  error_det[w,2] = 1
                  stop = T
                  break  # stop j loop
                }
              
                count_pair = count_pair + 1

            }
          
         }
    
        }
        
      }
      #######  End detection  ######
      
      doc_list[[l]] = error_det

    }

    cat("OCR",f,"Done! \n")
    Detection_list[[f]] = doc_list
  }
  
  return(Detection_list)
  
  #length(Detection_list[[1]])
  #Detection_list[[1]][[47]]
  #Detection_list[[1]][[44]]
  
}



#detection_ocr = Detection(OCR, digrams = dic_digrams)

#save(detection_ocr, file = paste0(project_path,"/output/detection_ocr.Rdata"))

load(paste0(project_path,"/output/detection_ocr.Rdata"))
# Check
length(detection_ocr)
head(detection_ocr[[1]],20)
tail(detection_ocr[[70]],20)

###### debug ######
astr <- "Ábcdêãçoàúü"
astr = c("lmageﬁaulldlng","avsfeåååååå")
iconv(astr, from = 'UTF-8', to = 'ASCII//TRANSLIT')

```

